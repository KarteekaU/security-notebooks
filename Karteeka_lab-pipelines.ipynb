{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e8ce3a-6393-4123-8588-f95278c827c3",
   "metadata": {},
   "source": [
    "# Lab -- Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfcdee7-c6bd-42ed-9106-9eed6fbf2409",
   "metadata": {},
   "source": [
    "In this assignment, you will modify an sklearn pipeline that classifies whether a url is a phish.\n",
    "You will:\n",
    "* modify a url processor to extract the path as its own column\n",
    "* enhance a domain feature transformer to extract additional domain features\n",
    "* create a path feature transformer to extract features based on the url path\n",
    "* add an additional pipeline to the provided ColumnTransformer that integrates your Path transformer\n",
    "\n",
    "Resources:\n",
    "\n",
    "* Much of the code in this started from https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html#classification-pipeline\n",
    "* Also see https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#examples-using-sklearn-pipeline-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0652071c-1ea7-4730-adc4-3a8d4d9d0443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cloudpickle\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf6b7b2b-4d29-456b-875b-b21670fbcfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74/3110187138.py:3: DtypeWarning: Columns (1,2,3,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('urlset.csv', encoding_errors='ignore', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "#!wget 'https://research.aalto.fi/files/16859732/urlset.csv.zip'\n",
    "#!unzip urlset.csv.zip\n",
    "df = pd.read_csv('urlset.csv', encoding_errors='ignore', on_bad_lines='skip')\n",
    "\n",
    "# select just the two columns we care about, and then drop na's before taking a train-test split\n",
    "df = df[['domain','label']]\n",
    "df = df.dropna()\n",
    "\n",
    "X, y = df[['domain']], df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2fd633-a819-4f65-8eee-c1c3e8cdf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that ex\n",
    "def url_processor(data):\n",
    "    ''' \n",
    "    Expects a pandas dataframe with column 'domain', where the domain actually is a full url. (The \n",
    "    source dataset calls it `domain`.) \n",
    "    \n",
    "    Returns a new dataframe with features extracted from the url. \n",
    "    '''\n",
    "    \n",
    "    # extract the domain as being everything before the first `/`\n",
    "    domain = data['domain'].str.split('/').str[0]\n",
    "    \n",
    "    # Add another column called \"path\" for everything after the first \"/\", but before any query string (before any `?`, if any)\n",
    "    #\n",
    "    # * Hint: There might be multiple `/` in the path, so you can't just modify the code above to take `.str[1]`.\n",
    "    #   To deal with this, see `n` argument here: https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html\n",
    "    # * Replace all null-values in with an empty string. \n",
    "    #   See https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html\n",
    "    \n",
    "    path = data['domain'].str.split('/',1).str[1].fillna('')\n",
    "    path = path.str.split('?').str[0]\n",
    "    \n",
    "    # DataFrame constructor accepts a dict of column-names mapped to pandas series.\n",
    "    return pd.DataFrame(\n",
    "        {'domain': domain, 'path': path}\n",
    "    )\n",
    "\n",
    "url_processor_transformer = FunctionTransformer(url_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095ab473-8831-47cc-9f9b-c1766b1b6ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_features(domains):\n",
    "    '''Expects a list of domains, and returns a list of dicts (one row per domain) with new features'''\n",
    "    return [\n",
    "        {\n",
    "            \"length\": len(domain),\n",
    "            \n",
    "            # add a second feature here counting the number of `.`'s in the domain\n",
    "            \"count_of_dots\": domain.count('.'),\n",
    "            \n",
    "            # add a third feature here for the domain's TLD (DictVectorizer will handle one-hot encoding for us)\n",
    "            \"tld\": domain.rsplit('.',1)[-1]\n",
    "        } for domain in domains\n",
    "    ]\n",
    "\n",
    "domain_features_transformer = FunctionTransformer(domain_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c132c80-4c72-4ca3-a360-9846c5173fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another transformer for the `path` feature. \n",
    "# Follow the pattern of the `domain_features` cell above\n",
    "\n",
    "def path_features(paths):\n",
    "    # create two features --\n",
    "    # * `path_length`, as the length of the `path\n",
    "    # * `count_of_slash, as the count of the number of `/` characters in the path\n",
    "    return [\n",
    "        {\n",
    "            \"path_length\": len(path),\n",
    "\n",
    "            # add a second feature here counting the number of `.`'s in the domain\n",
    "            \"count_of_slash\": path.count('/')\n",
    "        } for path in paths\n",
    "    ]\n",
    "\n",
    "path_features_transformer = FunctionTransformer(path_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e7180f-d856-4e70-9d16-be7aa63e5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ( \"prep\", url_processor_transformer),\n",
    "        # Use ColumnTransformer to combine the subject and body features\n",
    "        (\n",
    "            \"union\",\n",
    "            ColumnTransformer(\n",
    "                [\n",
    "                    # Pipeline for processing features from the 'domain' column\n",
    "                    (\n",
    "                        \"domain_features\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\n",
    "                                    \"features\",\n",
    "                                    domain_features_transformer,\n",
    "                                ),  # returns a list of dicts\n",
    "                                (\n",
    "                                    \"vect\",\n",
    "                                    DictVectorizer(),\n",
    "                                ),  # list of dicts -> feature matrix\n",
    "                            ]\n",
    "                        ),\n",
    "                        'domain',\n",
    "                    ),\n",
    "                    # Add another pipeline here for extracting features from the 'path' column.\n",
    "                    # Follow the example of the previous tuple in this list.\n",
    "                    \n",
    "                    (\n",
    "                        \"path_features\",\n",
    "                        Pipeline(\n",
    "                            [\n",
    "                                (\n",
    "                                    \"features\",\n",
    "                                    path_features_transformer,\n",
    "                                ),  # returns a list of dicts\n",
    "                                (\n",
    "                                    \"vect\",\n",
    "                                    DictVectorizer(),\n",
    "                                ),  # list of dicts -> feature matrix\n",
    "                            ]\n",
    "                        ),\n",
    "                        'path',\n",
    "                    ),\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        # Use a SVC classifier on the combined features\n",
    "        (\"svc\", LinearSVC(dual=False)),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e9bda9-77ea-47cf-af26-6e77944ba3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing prep, total=   1.4s\n",
      "[Pipeline] ............. (step 2 of 3) Processing union, total=   2.9s\n",
      "[Pipeline] ............... (step 3 of 3) Processing svc, total=   2.6s\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.86      0.80     11951\n",
      "         1.0       0.83      0.71      0.77     12028\n",
      "\n",
      "    accuracy                           0.79     23979\n",
      "   macro avg       0.79      0.79      0.78     23979\n",
      "weighted avg       0.79      0.79      0.78     23979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Classification report:\\n\\n{}\".format(classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136ac826-4875-4c4a-862e-34efca85aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle using CloudPickle, which will also pick up our transformer functions and whatnot\n",
    "# Append the current timestamp to the filename so I stop forgetting which one is the\n",
    "# most recent.\n",
    "\n",
    "timestamp = int(datetime.datetime.now().timestamp())\n",
    "\n",
    "with open(f'phish-model-{timestamp}.cloudpickle', 'wb') as f:\n",
    "    cloudpickle.dump(pipeline, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
